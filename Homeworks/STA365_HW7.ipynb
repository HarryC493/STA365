{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STA365 Homework 7"
      ],
      "metadata": {
        "id": "tqDfBTQ4Kw36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 8: (a) Posterior Predictive Distributions<br> and (b) Missing Data Imputation\n",
        "\n",
        "### 1. Describe how the posterior predictive distribution is created for mixture models\n",
        "\n",
        "### 2. Describe how the posterior predictive distribution is created in general\n",
        "\n",
        "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
        "\n",
        "- **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**\n",
        "\n",
        "### 4. Work on your course project\n",
        "\n"
      ],
      "metadata": {
        "id": "SUAofod7KwGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "Suppose we have specified our mixture model and obtained posterior distributions $P(\\boldsymbol\\theta, \\boldsymbol\\pi∣y)$ by sampling.\n",
        "\n",
        "- $\\boldsymbol \\theta$ represents the distribution parameters for random variable $Y$\n",
        "- $\\boldsymbol \\pi$ represents the proportion (weight) of each model.\n",
        "- $y$ represents the observed data.\n",
        "- Suppose we have k models with k parameters.\n",
        "\n",
        "We have a predictive model as the following:\n",
        "$P(\\tilde y | \\boldsymbol \\theta, \\boldsymbol \\pi) = \\sum_{k=1}^K \\pi_k f(\\tilde y | \\theta_k)$\n",
        "\n",
        "The predictive model captures the distribution of new data conditional on the parameters, and $\\tilde y$ represents the new data. Normally we would not have access to the new data, so we conduct random draws from the given data to create the new data, and assume the likelihood of the new data conditional on the parameters is the same as the likelihood of given data conditional on the parameters.\n",
        "\n",
        "To obtain the posterior predictive distribution $P(\\tilde y | y)$, essentially it is a double integral of the product of the predictive model and the posterior distribution:\n",
        "\n",
        "$P(\\tilde y | y) = \\int \\int P(\\tilde y | \\boldsymbol \\theta, \\boldsymbol \\pi) P(\\boldsymbol\\theta, \\boldsymbol\\pi∣y) d\\boldsymbol \\pi d\\boldsymbol \\theta$\n",
        "\n",
        "Due to its complexity and being computational expensive, we would not directly compute this integral, but using sampling methods such as Markov Chain Monte Carlo to approximate the true posterior predictive distribution. So essentially we are creating the posterior predictive distribution by sampling from the joint posterior distribution $P(\\tilde y,\\boldsymbol \\theta, \\boldsymbol \\pi | y)$."
      ],
      "metadata": {
        "id": "Mukgvzj-K6Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "\n",
        "In general, a posterior predictive distribution is the following:\n",
        "\n",
        "$\n",
        "P(\\tilde y | y) = \\int P(\\tilde y | \\theta) P(\\theta | y) d\\theta \\propto \\int P(\\tilde y, \\theta | y) d\\theta\n",
        "$\n",
        "\n",
        "- $P(\\tilde y | y)$ is the posterior predictive distribution, where $\\tilde y$ represents new data not included in model fitting. $P(\\tilde y | y)$ is the distribution of the new data conditional on the observed data $y$. It is called the predictive distribution because we are using observed data to predict the distribution of new data.\n",
        "- $\\theta$ is a distribution parameter for some random variable $Y$\n",
        "- $P(\\theta | y)$ is the posterior distribution of $\\theta$ given the observed data $y$\n",
        "  - By Bayes theorem, $P(\\theta | y) \\propto P(y|\\theta) P(\\theta)$\n",
        "  - $P(y|\\theta)$ is the likelihood from data\n",
        "  - $P(\\theta)$ is the prior on $\\theta$\n",
        "- $P(\\tilde y | \\theta)$ is called the predictive model, where it captures the distribution of new data conditional on the parameter. Likewise, ususally it is assumed to be same as the likelihood of the given data given its parameters.\n",
        "- Note that $P(\\tilde y | \\theta) P(\\theta | y)$ can be reduced to joint posterior distribution $P(\\tilde y, \\theta | y)$\n",
        "\n",
        "To obtain the posterior predictive distribution $P(\\tilde y | y)$, we would usually sample from the joint posterior distribution using sampling methods such as Markov Chain Monte Carlo in order to avoid direct computation of the integral.\n",
        "\n",
        "For other cases, such as a regression, we simply add the predictors into our conditionals and replace parameters as regression coefficients."
      ],
      "metadata": {
        "id": "2cN7pV56LaoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3\n",
        "\n",
        "First, I would have to make a wild, unverifiable assumption on how the data is missing. I have the following options:\n",
        "- Data is missing due to complete randomness (MCAR)\n",
        "- Data is missing due to some observed factors (MAR)\n",
        "- Data is missing due to some unobserved factors (MNAR)\n",
        "\n",
        "I cannot impute the missing values with the third assumption since I do not have data for those unobserved factors, but I can conduct the Bayesian analysis with the first two analysis by imputing those missing values from the posterior predictive distribution.\n",
        "\n",
        "For the first case, I can impute the values by drawing some random numbers from a uniform distribution to imitate the \"missing at completely random\" nature, and carry on with my analysis.\n",
        "\n",
        "For the second case, I can impute the values by regressing the variable that contains missing value on other variables that does not include missing values, and use the imputed dataset to carry on with my analysis."
      ],
      "metadata": {
        "id": "v3Dyws__W8Ar"
      }
    }
  ]
}